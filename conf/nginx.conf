daemon off;
worker_processes auto;
pid /var/lib/hypothesis/nginx.pid;
error_log /dev/stderr;

events {
  worker_connections 4096;
}

http {
  client_max_body_size 20m;
  sendfile on;
  server_tokens off;

  include mime.types;
  default_type application/octet-stream;

  access_log off;

  # If there is an auth token, rate limit based on that,
  # otherwise rate limit per ip.
  map $cookie_auth$http_bearer $limit_key {
    default $binary_remote_addr;
    ~.* $cookie_auth$http_bearer;
  }

  # Make a rate limit per cookie token, bearer token, 
  # and ip address of 3 requests/minute max 12 requests/minute.
  limit_req_zone $limit_key zone=user_12rpm_limit:12m rate=3r/m;
  

  # We set fail_timeout=0 so that the upstream isn't marked as down if a single
  # request fails (e.g. if gunicorn kills a worker for taking too long to handle
  # a single request).
  upstream web { server unix:/tmp/gunicorn-web.sock fail_timeout=0; }
  upstream websocket { server unix:/tmp/gunicorn-websocket.sock fail_timeout=0; }

  server {
    listen 5000;

    server_name _;
    server_tokens off;

    root /var/www;

    # Each /api/search request takes on average ~90ms
    # and we have an aproximate maximum of 16 active
    # users per minute. Assuming think time is 5 seconds
    # the max request rate per user = 12 requests/min.
    #   (60 s/min) / (5.090 s/request)
    #
    # Each /api/search request has an expected request
    # rate of 3 requests/min per user (based on current
    # traffic).
    #
    # In bursty machine traffic, assuming a bot think
    # time of .02s and 16 users per minute would have
    # a request rate of 38 requests/min.
    #   (60 s/min) / (0.110 s/request) / (16 users/min)
    # It's worth considering that not each user will be
    # requesting such a large amount so it may be
    # reasonable to let a user exceed that value.
    #
    # With a burst of 26 requests, a given user can
    # queue up 27 requests, and then 1 more request
    # each 5 seconds there after. If the incoming
    # requests overflow the queue, they are dropped
    # with a 429 response.
    #   queue size  = max rate - number wait periods
    #   26 requests = 38 requests/min - 12 requests
    location /api/search {
      limit_req zone=user_12rpm_limit burst=26 nodelay;
      limit_req_status 429;

      proxy_pass http://web;
    }

    # Each /api/annotations request takes on average ~310ms.
    # and we have on average < 1 request/user/second.
    #location /api/annotations {
    #    limit_req zone=user_2rps_limit burst=1 nodelay;

    #    proxy_pass http://hypothes.is/api/search;
    #}

    location /ws {
      proxy_pass http://websocket;
      proxy_http_version 1.1;
      proxy_redirect off;
      proxy_buffering off;
      proxy_set_header Upgrade $http_upgrade;
      proxy_set_header Connection upgrade;
      proxy_set_header Host $host;
      proxy_set_header X-Forwarded-Server $http_host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

    location /annotating-all-knowledge/ {
      proxy_pass https://hypothesis.github.io;
      proxy_http_version 1.1;
      proxy_set_header Host $proxy_host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

    location /roadmap {
      return 302 "https://trello.com/b/2ajZ2dWe/public-roadmap";
    }

    location / {
      proxy_pass http://web;
      proxy_http_version 1.1;
      proxy_connect_timeout 10s;
      proxy_send_timeout 10s;
      proxy_read_timeout 10s;
      proxy_redirect off;
      proxy_set_header Host $host;
      proxy_set_header X-Forwarded-Server $http_host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Request-Start "t=${msec}";
    }
  }

  server {
    listen 127.0.0.234:5000;
    server_name _;

    location /status {
      stub_status on;
      access_log off;
      allow 127.0.0.0/24;
      deny all;
    }
  }
}
